{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll30IcHiBg1W"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install sacrebleu\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jenE9SpTAF1i",
        "outputId": "0d7fab45-9d37-4b9c-d47e-3bce5cf19ebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-16 14:08:51--  https://raw.githubusercontent.com/divar167/data/refs/heads/main/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 74015 (72K) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]  72.28K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-16 14:08:52 (712 KB/s) - ‘train.txt’ saved [74015/74015]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.7976017618179325\n",
            "Epoch 2, Loss: 3.479969654083252\n",
            "Epoch 3, Loss: 2.7090363693237305\n",
            "Epoch 4, Loss: 2.1963003063201905\n",
            "Epoch 5, Loss: 1.7973324251174927\n",
            "Epoch 6, Loss: 1.4727373778820039\n",
            "Epoch 7, Loss: 1.2140832459926605\n",
            "Epoch 8, Loss: 0.9812359869480133\n",
            "Epoch 9, Loss: 0.8077787256240845\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# Dataset Parsing\n",
        "SEPARATOR = '<sep>'\n",
        "# Download the training set.\n",
        "!wget https://raw.githubusercontent.com/divar167/data/refs/heads/main/train.txt -O train.txt\n",
        "with open('train.txt') as file:\n",
        "    train = [line.rstrip() for line in file]\n",
        "\n",
        "# Unicode normalization\n",
        "def normalize_unicode(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "    s = normalize_unicode(s)\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    s = s.strip()\n",
        "    return s\n",
        "\n",
        "# Filter and preprocess the dataset\n",
        "train_filtered = [pair for pair in train if SEPARATOR in pair and pair.strip() != '']\n",
        "train_input, train_target = map(list, zip(*[pair.split(SEPARATOR) for pair in train_filtered]))\n",
        "train_input = [preprocess_sentence(sentence.strip()) for sentence in train_input]\n",
        "train_target = [preprocess_sentence(sentence.strip()) for sentence in train_target]\n",
        "\n",
        "# Vocabulary Creation\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<sos>\": 0, \"<eos>\": 1, \"<unk>\": 2}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab(train_input)\n",
        "tgt_vocab = build_vocab(train_target)\n",
        "rev_tgt_vocab = {idx: token for token, idx in tgt_vocab.items()}\n",
        "\n",
        "# Dataset Class\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=50):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_sentences[idx]\n",
        "        tgt = self.tgt_sentences[idx]\n",
        "        return src, tgt\n",
        "\n",
        "# Collate function to pad sequences\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    # Convert lists to tensors and add SOS and EOS tokens\n",
        "    src_batch = [torch.tensor([src_vocab[\"<sos>\"]] + [src_vocab.get(token, src_vocab[\"<unk>\"]) for token in sentence.split()] + [src_vocab[\"<eos>\"]]) for sentence in src_batch]\n",
        "    tgt_batch = [torch.tensor([tgt_vocab[\"<sos>\"]] + [tgt_vocab.get(token, tgt_vocab[\"<unk>\"]) for token in sentence.split()] + [tgt_vocab[\"<eos>\"]]) for sentence in tgt_batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab[\"<unk>\"])\n",
        "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab[\"<unk>\"])\n",
        "\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TranslationDataset(train_input, train_target, src_vocab, tgt_vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Translation Model\n",
        "class TranslationModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, num_heads, num_layers, ff_dim):\n",
        "        super(TranslationModel, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, embed_dim)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embed_dim)\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim), num_layers\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(embed_dim, num_heads, ff_dim), num_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_key_padding_mask = (src == src_vocab[\"<unk>\"])  # Mask for padding tokens\n",
        "        tgt_key_padding_mask = (tgt == tgt_vocab[\"<unk>\"])  # Mask for padding tokens\n",
        "\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "        tgt_mask = nn.Transformer().generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
        "\n",
        "        src_emb = self.encoder_embedding(src)\n",
        "        tgt_emb = self.decoder_embedding(tgt)\n",
        "\n",
        "        memory = self.encoder(\n",
        "            src_emb.transpose(0, 1),\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        output = self.decoder(\n",
        "            tgt_emb.transpose(0, 1),\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask\n",
        "        )\n",
        "        return self.fc(output.transpose(0, 1))\n",
        "\n",
        "# Model Parameters\n",
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "ff_dim = 4096\n",
        "\n",
        "model = TranslationModel(src_vocab_size, tgt_vocab_size, embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Training Loop\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<unk>\"])\n",
        "optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.reshape(-1, tgt_vocab_size)\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "        loss = criterion(output, tgt_output)\n",
        "        epoch_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(dataloader)}\")\n",
        "\n",
        "# Translation Function\n",
        "def translate(sentence, model, src_vocab, tgt_vocab, rev_tgt_vocab, max_len=50):\n",
        "    model.eval()\n",
        "    tokens = [src_vocab.get(t, src_vocab[\"<unk>\"]) for t in sentence.split()]\n",
        "    src = torch.tensor([src_vocab[\"<sos>\"]] + tokens + [src_vocab[\"<eos>\"]]).unsqueeze(0)\n",
        "    tgt = torch.tensor([tgt_vocab[\"<sos>\"]]).unsqueeze(0)\n",
        "\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output = model(src, tgt)\n",
        "            next_token = output.argmax(-1)[:, -1].item()\n",
        "            outputs.append(next_token)\n",
        "            if next_token == tgt_vocab[\"<eos>\"]:\n",
        "                break\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token]])], dim=1)\n",
        "\n",
        "    return \" \".join(rev_tgt_vocab[t] for t in outputs[:-1])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Test the Model\n",
        "test_sentence = [\n",
        "    \"Adam Dârayavauš xšâyathiya vazraka\",\n",
        "    \"aivam parûvnâm xšâyathiyam \",\n",
        "    \"hya šiyâtim adâ martiyahyâ \",\n",
        "    \"hya martiyam adâ\",\n",
        "    \"hauv xšâyathiya abava\",\n",
        "    \"hya avam asmânam adâ\",\n",
        "    \"šiyâtim hauv agarbâyatâ vavam\",\n",
        "    \"Yadiy Pârsa pâta ahatiy\",\n",
        "    \"Ariya Ariya ciça\"\n",
        "]\n",
        "data_translation = [\n",
        "    \"I am Darius the great king\",\n",
        "    \"one king for many\",\n",
        "    \"who created happiness for man \",\n",
        "    \"who created man \",\n",
        "    \"he became king\",\n",
        "    \"who created yonder sky \",\n",
        "    \"he seized the happiness of my family\",\n",
        "    \"If the Persian people are protected\",\n",
        "    \"an Aryan of Aryan descent  \"\n",
        "\n",
        "]\n",
        "\n",
        "# Initialize table\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Sentence\", \"Data Translation\", \"Model Translation\"]\n",
        "\n",
        "# Generate translations and populate the table\n",
        "for s, dt in zip(test_sentence, data_translation):\n",
        "    translation = translate(s, model, src_vocab, tgt_vocab, rev_tgt_vocab)\n",
        "    table.add_row([s.strip(), dt.strip(), translation.strip()])\n",
        "\n",
        "# Print the table\n",
        "print(table)\n"
      ],
      "metadata": {
        "id": "SiPDorJolVcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import unicodedata\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset Parsing\n",
        "SEPARATOR = '<sep>'\n",
        "!wget https://raw.githubusercontent.com/divar167/data/refs/heads/main/train.txt -O train.txt\n",
        "with open('train.txt') as file:\n",
        "    train = [line.rstrip() for line in file]\n",
        "\n",
        "# Unicode normalization\n",
        "def normalize_unicode(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "    s = normalize_unicode(s)\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    s = s.lower()\n",
        "    s = s.strip()\n",
        "    return s\n",
        "\n",
        "# Filter and preprocess the dataset\n",
        "train_filtered = [pair for pair in train if SEPARATOR in pair and pair.strip() != '']\n",
        "train_input, train_target = map(list, zip(*[pair.split(SEPARATOR) for pair in train_filtered]))\n",
        "train_input = [preprocess_sentence(sentence.strip()) for sentence in train_input]\n",
        "train_target = [preprocess_sentence(sentence.strip()) for sentence in train_target]\n",
        "\n",
        "# Vocabulary Creation\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<sos>\": 0, \"<eos>\": 1, \"<unk>\": 2}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab(train_input)\n",
        "tgt_vocab = build_vocab(train_target)\n",
        "rev_tgt_vocab = {idx: token for token, idx in tgt_vocab.items()}\n",
        "\n",
        "# Dataset Class\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=50):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.src_sentences[idx]\n",
        "        tgt = self.tgt_sentences[idx]\n",
        "        return src, tgt\n",
        "\n",
        "# Collate function to pad sequences\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_batch = [torch.tensor([src_vocab[\"<sos>\"]] + [src_vocab.get(token, src_vocab[\"<unk>\"]) for token in sentence.split()] + [src_vocab[\"<eos>\"]]) for sentence in src_batch]\n",
        "    tgt_batch = [torch.tensor([tgt_vocab[\"<sos>\"]] + [tgt_vocab.get(token, tgt_vocab[\"<unk>\"]) for token in sentence.split()] + [tgt_vocab[\"<eos>\"]]) for sentence in tgt_batch]\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=src_vocab[\"<unk>\"])\n",
        "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab[\"<unk>\"])\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_input, val_input, train_target, val_target = train_test_split(\n",
        "    train_input, train_target, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Create datasets and dataloaders for training and validation\n",
        "train_dataset = TranslationDataset(train_input, train_target, src_vocab, tgt_vocab)\n",
        "val_dataset = TranslationDataset(val_input, val_target, src_vocab, tgt_vocab)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Translation Model\n",
        "class TranslationModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim, num_heads, num_layers, ff_dim, dropout=0.2):\n",
        "        super(TranslationModel, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, embed_dim)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embed_dim)\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout=dropout), num_layers\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(embed_dim, num_heads, ff_dim, dropout=dropout), num_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_key_padding_mask = (src == src_vocab[\"<unk>\"])\n",
        "        tgt_key_padding_mask = (tgt == tgt_vocab[\"<unk>\"])\n",
        "\n",
        "        tgt_seq_len = tgt.size(1)\n",
        "        tgt_mask = nn.Transformer().generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
        "\n",
        "        src_emb = self.encoder_embedding(src)\n",
        "        tgt_emb = self.decoder_embedding(tgt)\n",
        "\n",
        "        memory = self.encoder(\n",
        "            src_emb.transpose(0, 1),\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        output = self.decoder(\n",
        "            tgt_emb.transpose(0, 1),\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask\n",
        "        )\n",
        "        return self.fc(output.transpose(0, 1))\n",
        "\n",
        "# Model Parameters\n",
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "embed_dim = 1024\n",
        "num_heads = 16\n",
        "num_layers = 8\n",
        "ff_dim = 4096\n",
        "dropout = 0.2\n",
        "\n",
        "model = TranslationModel(src_vocab_size, tgt_vocab_size, embed_dim, num_heads, num_layers, ff_dim, dropout)\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = AdamW(model.parameters(), lr=0.00005, weight_decay=1e-6)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<unk>\"])\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for src, tgt in train_dataloader:\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.reshape(-1, tgt_vocab_size)\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "        loss = criterion(output, tgt_output)\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_dataloader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, tgt_vocab_size)\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "            loss = criterion(output, tgt_output)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    scheduler.step(val_loss / len(val_dataloader))\n",
        "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss / len(train_dataloader):.4f}, Val Loss: {val_loss / len(val_dataloader):.4f}\")\n",
        "\n",
        "# Translation Function\n",
        "def translate2(sentence, model, src_vocab, tgt_vocab, rev_tgt_vocab, max_len=50):\n",
        "    model.eval()\n",
        "    tokens = [src_vocab.get(t, src_vocab[\"<unk>\"]) for t in sentence.split()]\n",
        "    src = torch.tensor([src_vocab[\"<sos>\"]] + tokens + [src_vocab[\"<eos>\"]]).unsqueeze(0)\n",
        "    tgt = torch.tensor([tgt_vocab[\"<sos>\"]]).unsqueeze(0)\n",
        "\n",
        "    outputs = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            output = model(src, tgt)\n",
        "            next_token = output.argmax(-1)[:, -1].item()\n",
        "            outputs.append(next_token)\n",
        "            if next_token == tgt_vocab[\"<eos>\"]:\n",
        "                break\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token]])], dim=1)\n",
        "\n",
        "    return \" \".join(rev_tgt_vocab[t] for t in outputs[:-1])\n"
      ],
      "metadata": {
        "id": "3d_ktg7EXT5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed800021-9471-40e6-f4d0-c7d10b3cd29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-19 04:35:16--  https://raw.githubusercontent.com/divar167/data/refs/heads/main/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72818 (71K) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]  71.11K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-19 04:35:16 (4.56 MB/s) - ‘train.txt’ saved [72818/72818]\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 5.0974, Val Loss: 4.6235\n",
            "Epoch 2, Train Loss: 4.2667, Val Loss: 3.8892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Test the Model\n",
        "test_sentence = [\n",
        "    \"Adam Dârayavauš xšâyathiya vazraka\",\n",
        "    \"aivam parûvnâm xšâyathiyam \",\n",
        "    \"hya šiyâtim adâ martiyahyâ \",\n",
        "    \"hya martiyam adâ\",\n",
        "    \"hauv xšâyathiya abava\",\n",
        "    \"hya avam asmânam adâ\",\n",
        "    \"šiyâtim hauv agarbâyatâ vavam\",\n",
        "    \"Yadiy Pârsa pâta ahatiy\",\n",
        "    \"Ariya Ariya ciça\"\n",
        "]\n",
        "data_translation = [\n",
        "    \"I am Darius the great king\",\n",
        "    \"one king for many\",\n",
        "    \"who created happiness for man \",\n",
        "    \"who created man \",\n",
        "    \"he became king\",\n",
        "    \"who created yonder sky \",\n",
        "    \"he seized the happiness of my family\",\n",
        "    \"If the Persian people are protected\",\n",
        "    \"an Aryan of Aryan descent  \"\n",
        "\n",
        "]\n",
        "\n",
        "# Initialize table\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Sentence\", \"Data Translation\", \"Model Translation\"]\n",
        "\n",
        "# Generate translations and populate the table\n",
        "output_rows = []\n",
        "for s, dt in zip(test_sentence, data_translation):\n",
        "    translation = translate2(s, model, src_vocab, tgt_vocab, rev_tgt_vocab)\n",
        "    row = [s.strip(), dt.strip(), translation.strip()]\n",
        "    table.add_row(row)\n",
        "    output_rows.append(row)\n",
        "\n",
        "# # Print the table in the notebook\n",
        "print(table)\n",
        "\n",
        "# # Save the table to a file\n",
        "# with open(\"translations.txt\", \"w\") as file:\n",
        "#     file.write(str(table))\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download(\"translations.txt\")\n"
      ],
      "metadata": {
        "id": "jR9EIgZwXWpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94186a90-c79b-47cb-a8b1-77454e75d1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+--------------------------------------+--------------------------------+\n",
            "|              Sentence              |           Data Translation           |       Model Translation        |\n",
            "+------------------------------------+--------------------------------------+--------------------------------+\n",
            "| Adam Dârayavauš xšâyathiya vazraka |      I am Darius the great king      |   I am Darius the great king   |\n",
            "|     aivam parûvnâm xšâyathiyam     |          one king for many           |       one ruler of many        |\n",
            "|     hya šiyâtim adâ martiyahyâ     |    who created happiness for man     |          who created           |\n",
            "|          hya martiyam adâ          |           who created man            |        who created man         |\n",
            "|       hauv xšâyathiya abava        |            he became king            |         He became king         |\n",
            "|        hya avam asmânam adâ        |        who created yonder sky        |     who created yonder sky     |\n",
            "|   šiyâtim hauv agarbâyatâ vavam    | he seized the happiness of my family | he seized the son of my family |\n",
            "|      Yadiy Pârsa pâta ahatiy       | If the Persian people are protected  |         If the Persian         |\n",
            "|          Ariya Ariya ciça          |      an Aryan of Aryan descent       |   an Aryan of Aryan an Aryan   |\n",
            "+------------------------------------+--------------------------------------+--------------------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}